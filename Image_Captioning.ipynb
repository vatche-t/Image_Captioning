{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "import torchvision\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DATA: https://uofi.app.box.com/s/1cpolrtkckn4hxr1zhmfg0ln9veo6jpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMAGES_DIR = \"flickr30k-images\"\n",
    "LABEL_PATH = \"flickr30k-images/results.csv\"\n",
    "OUTPUT_PATH = \"\"\n",
    "\n",
    "# Some special tokens for RNN model...\n",
    "UNK = \"#UNK\"\n",
    "PAD = \"#PAD\"\n",
    "START = \"#START\"\n",
    "END = \"#END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(LABEL_PATH, sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def clean_text(row):\n",
    "    row = str(row).strip()\n",
    "    row = row.lower()\n",
    "    return regex.sub(\"\", row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [col.strip() for col in df.columns]\n",
    "df[\"comment\"] = df[\"comment\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"length\"] = df[\"comment\"].apply(lambda row: len(row.strip().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = df[\"comment\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "for caption in captions:\n",
    "    caption = caption.strip()\n",
    "    for word in caption.split():\n",
    "        if word not in word_freq:\n",
    "            word_freq[word] = 0\n",
    "        word_freq[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(sorted(word_freq.items(), key=lambda item: item[1])[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(sorted(word_freq.items(), key=lambda item: item[1], reverse=True)[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(captions, word_freq, count_threshold=5):\n",
    "    \"\"\"\n",
    "    This function builds `vocab` dictionary from list of text captions.\n",
    "    Also, add constant PAD, UNK, START, END to `vocab`.\n",
    "    Add a word to vocab if its occurence frequency is larger than `count_threshold`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    captions: a list of preprocessed text captions above.\n",
    "    word_freq: a dictionary of word occurence frequency.\n",
    "    count_threshold: a int to use when building vocab.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocab: an dictionary vocabulary of key-value pair which is:\n",
    "        -> key: string text\n",
    "        -> value:  token index\n",
    "    inv_vocab: an inverse dictionary vocabulary of key-value pair which is:\n",
    "        -> key: token index\n",
    "        -> value: string text\n",
    "        \n",
    "    E.g: vocab = {\"two\": 4, \"young\": 5, \"guys\": 6, ...} \n",
    "         inv_vocab = {4: \"two\", 5: \"young\", 6: \"guys\", ...}\n",
    "    \"\"\"\n",
    "    vocab = {\n",
    "        PAD: 0,\n",
    "        UNK: 1,\n",
    "        START: 2,\n",
    "        END: 3\n",
    "    }\n",
    "    index = 4\n",
    "    \n",
    "    for caption in captions:\n",
    "        caption = caption.strip().split(\" \")\n",
    "        for word in caption:\n",
    "            if word and word_freq[word] >= count_threshold and word not in vocab:\n",
    "                vocab[word] = index\n",
    "                index += 1\n",
    "\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, inv_vocab = build_vocab(captions, word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_captions(captions, vocab, max_length=30):\n",
    "    \"\"\"\n",
    "    Convert text captions to index token based on `vocab`.\n",
    "    If a word not in vocab, replace it by the token index of `UNK` constant.\n",
    "    Also, add `START` constant to the beginning of the sentence and \n",
    "            `END` constant to the end of the sentence.\n",
    "    After add `START` and `END` constant, if the length is still < 30,\n",
    "        use `PAD` constant to fill remaining positions.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    captions: a list of preprocessed text captions above.\n",
    "    vocab: a dictionary vocabulary of key-value pair which is:\n",
    "        -> key: string text\n",
    "        -> value: token index\n",
    "    max_length: an int denotes fixed maximum length to the captions.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tokens: a list of tokens get from `vocab`\n",
    "    \"\"\"\n",
    "    tokens = [[vocab[PAD]]*max_length for _ in range(len(captions))]\n",
    "    for i, caption in enumerate(captions):\n",
    "        caption = caption.strip().split()\n",
    "        tokens[i][0] = vocab[START]\n",
    "        j = 1\n",
    "        for word in caption[:max_length-2]:\n",
    "            if word not in vocab:\n",
    "                tokens[i][j] = vocab[UNK]\n",
    "            else:\n",
    "                tokens[i][j] = vocab[word]\n",
    "            j += 1\n",
    "        tokens[i][j] = vocab[END]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = convert_captions(captions, vocab)\n",
    "img_paths = list(df[\"image_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, img_paths, tokens):\n",
    "        \"\"\"\n",
    "        img_paths: a list of image path we get from dataframe\n",
    "        tokens: a list of tokens that we converted from text captions\n",
    "        \"\"\"\n",
    "        self.img_paths = [os.path.join(INPUT_IMAGES_DIR, p) for p in img_paths]\n",
    "        self.tokens = tokens\n",
    "        assert len(self.img_paths) == len(self.tokens), \"Make sure len(img_paths) == len(tokens).\"\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get image path and token. Then load image path to numpy array image. Convert to pytorch tensor if it's necessary. \n",
    "        \"\"\"\n",
    "        img_path = self.img_paths[index]\n",
    "        token = self.tokens[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = self._resize_img(img, shape=(300, 300))\n",
    "        img = torchvision.transforms.ToTensor()(img)\n",
    "        token = torch.as_tensor(token)\n",
    "        return img, token\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def _resize_img(self, img, shape=(300, 300)):\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        pad_left = 0\n",
    "        pad_right = 0\n",
    "        pad_top = 0\n",
    "        pad_bottom = 0\n",
    "        if h > w:\n",
    "            diff = h - w\n",
    "            pad_top = diff - diff // 2\n",
    "            pad_bottom = diff // 2\n",
    "        else:\n",
    "            diff = w - h\n",
    "            pad_left = diff - diff // 2\n",
    "            pad_right = diff // 2\n",
    "        cropped_img = img[pad_top:h-pad_bottom, pad_left:w-pad_right, :]\n",
    "        cropped_img = cv2.resize(cropped_img, shape)\n",
    "        return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageCaptioningDataset(img_paths, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Captioning Model](https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/03-advanced/image_captioning/png/model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30\n",
    "NUM_VOCAB = len(vocab)\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Define CNN encoder class:\n",
    "\n",
    "The best practice is to use pretrained models from ImageNet: VGG, Resnet, Alexnet, Googlenet,... We can call those pretrained models are the backbones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.cnn = torchvision.models.resnet34(pretrained=True)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.cnn(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Define LSTM decoder class:\n",
    "\n",
    "In this class, you should have to define nn.Embedding, nn.LSTM, nn.Linear,... to appropriate training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, decoder_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, decoder_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(decoder_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, decoder_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, decoder_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "\n",
    "        return attention_weighted_encoding, alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_vocab) -> None:\n",
    "        super().__init__()\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.num_vocab = num_vocab\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_vocab, embedding_dim=256, padding_idx=0)\n",
    "        self.num_layers = 1\n",
    "        self.bidirectional = False\n",
    "        self.rnn = nn.LSTM(input_size=256, hidden_size=256, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, num_vocab)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input, img_embeded, prediction=False):\n",
    "        img_embeded = self.bottleneck(img_embeded)\n",
    "        img_embeded = torch.stack([img_embeded]*(self.num_layers), dim=0)\n",
    "        if prediction:\n",
    "            output = []\n",
    "            hidden = (img_embeded, img_embeded)\n",
    "            out = input\n",
    "            while out != vocab[END] and len(output) <= MAX_LENGTH:\n",
    "                out = torch.tensor([[out]]).to(\"cuda\")\n",
    "                out = self.embedding(out)\n",
    "                out, hidden = self.rnn(out, hidden)\n",
    "                out = self.classifier(out)\n",
    "                out = self.softmax(out)\n",
    "                out = torch.argmax(out, dim=-1)\n",
    "                out = out.squeeze().item()\n",
    "                output.append(out)\n",
    "        else:\n",
    "            input = self.embedding(input)\n",
    "            output, (h, c) = self.rnn(input, (img_embeded, img_embeded))\n",
    "            output = self.classifier(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel:\n",
    "\n",
    "    def __init__(self, encoder : CNNEncoder, decoder : RNNDecoder, train_dataset : ImageCaptioningDataset):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.encoder = encoder.to(self.device)\n",
    "        self.encoder.eval()\n",
    "        self.decoder = decoder.to(self.device)\n",
    "        self.train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        self.optimizer = optim.Adam(decoder.parameters())\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def predict(self, img):\n",
    "        with torch.no_grad():\n",
    "            img_embed = self.encoder(img)\n",
    "            caption = vocab[START]\n",
    "            caption = self.decoder(caption, img_embed, prediction=True)\n",
    "        \n",
    "        text = [inv_vocab[t] for t in caption]\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "    \n",
    "    def train(self):\n",
    "        for e in range(EPOCH):\n",
    "            pbar = tqdm(self.train_dataloader, desc=\"Epoch: {}\".format(e+1))\n",
    "            for i, (img, caption) in enumerate(pbar):\n",
    "                img = img.to(self.device)\n",
    "                caption = caption.to(self.device)\n",
    "                img_embed = self.encoder(img)\n",
    "                output = self.decoder(caption[:, :-1], img_embed)\n",
    "                output = output.permute(0, 2, 1)\n",
    "                loss = self.loss(output, caption[:, 1:])\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward() \n",
    "                self.optimizer.step()\n",
    "\n",
    "                pbar.set_description(desc=\"Epoch \" + str(e+1) + \" - Loss: %.5f\" % (loss.item()))\n",
    "                \n",
    "                if ((i+1)%100) == 0:\n",
    "                    plt.imshow(img[-1].cpu().detach().numpy().transpose((1, 2, 0)))\n",
    "                    output = self.predict(img[-1].unsqueeze(0))\n",
    "                    plt.title(output)\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNNEncoder()\n",
    "rnn = RNNDecoder(num_vocab=NUM_VOCAB)\n",
    "model = ImageCaptioningModel(encoder=cnn, decoder=rnn, train_dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge import Rouge\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predicted_captions = []\n",
    "    references = []\n",
    "    for img, true_caption in dataloader:\n",
    "        img = img.to(device)\n",
    "        predicted_caption = model.predict(img)\n",
    "        predicted_captions.append(predicted_caption)\n",
    "        references.append([true_caption])\n",
    "\n",
    "    # Calculate BLEU, METEOR, and ROUGE\n",
    "    bleu_score = corpus_bleu(references, predicted_captions)\n",
    "    meteor_scores = [meteor_score([ref], pred) for ref, pred in zip(references, predicted_captions)]\n",
    "    meteor_avg = sum(meteor_scores) / len(meteor_scores)\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(predicted_captions, references)\n",
    "    rouge_avg = sum([score['rouge-l']['f'] for score in rouge_scores]) / len(rouge_scores)\n",
    "\n",
    "    return bleu_score, meteor_avg, rouge_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet demonstrates how to add regularization, set training parameters like learning rate, batch size, and how to perform cross-validation. Remember to replace features, captions, vocab_size, embedding_dim, and max_length with your actual dataset and model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
